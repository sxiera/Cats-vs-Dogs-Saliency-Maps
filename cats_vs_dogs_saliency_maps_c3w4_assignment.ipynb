{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQiSujBfjWj"
      },
      "source": [
        "# **Final Week Bima Aristo**\n",
        "\n",
        "A saliency map shows the pixels which greatly impacts the classification of an image.\n",
        "- This is done by getting the gradient of the loss with respect to changes in the pixel values, then plotting the results.\n",
        "- From there, you can see if your model is looking at the correct features when classifying an image.\n",
        "  - For example, if you're building a dog breed classifier, you should be wary if your saliency map shows strong pixels outside the dog itself (e.g. sky, grass, dog house, etc...).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDHISSfBq40T"
      },
      "source": [
        "### Download test files and weights\n",
        "\n",
        "Let's begin by first downloading files we will be using for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Laatr1c6lr1w"
      },
      "outputs": [],
      "source": [
        "# Download the same test files from the Cats vs Dogs ungraded lab\n",
        "!wget -O cat1.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/cat1.jpeg\n",
        "!wget -O cat2.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/cat2.jpeg\n",
        "!wget -O catanddog.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/catanddog.jpeg\n",
        "!wget -O dog1.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/dog1.jpeg\n",
        "!wget -O dog2.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/dog2.jpeg\n",
        "\n",
        "# Download prepared weights\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kipXTxesGJKGY1B8uSPRvxROgOH90fih' -O 0_epochs.h5\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1oiV6tjy5k7h9OHGTQaf0Ohn3FmF-uOs1' -O 15_epochs.h5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24L3lKwqb3E"
      },
      "source": [
        "### Import the required packages\n",
        "\n",
        "Please import:\n",
        "\n",
        "  * Tensorflow\n",
        "  * Tensorflow Datasets\n",
        "  * Numpy\n",
        "  * Matplotlib's PyPlot\n",
        "  * Keras Models API classes you will be using\n",
        "  * Keras layers you will be using\n",
        "  * OpenCV (cv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X86LKLvpBO2S"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th4dA3I8-9Ue"
      },
      "source": [
        "### Download and prepare the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1hujOK9rDyU"
      },
      "source": [
        "#### Load Cats vs Dogs\n",
        "\n",
        "* Required: Use Tensorflow Datasets to fetch the `cats_vs_dogs` dataset.\n",
        "  * Use the first 80% of the *train* split of the said dataset to create your training set.\n",
        "  * Set the `as_supervised` flag to create `(image, label)` pairs.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w5HNdoHBQv_"
      },
      "outputs": [],
      "source": [
        "# Load the data and create the train set (optional: val and test sets)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "splits = [\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"]\n",
        "\n",
        "splits, info = tfds.load(name=\"cats_vs_dogs\", with_info=True, as_supervised=True, split=splits)\n",
        "\n",
        "(train_examples, validation_examples, test_examples) = splits\n",
        "\n",
        "num_examples = info.splits[\"train\"].num_examples\n",
        "num_classes = info.features[\"label\"].num_classes\n",
        "\n",
        "num_examples, num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXp0mV5Rbo76"
      },
      "source": [
        "#### Create preprocessing function\n",
        "\n",
        "Define a function that takes in an image and label. This will:\n",
        "  * cast the image to float32\n",
        "  * normalize the pixel values to [0, 1]\n",
        "  * resize the image to 300 x 300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRkrL2aK2_UZ"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (300, 300)\n",
        "def augmentimages(image, label):\n",
        "  # YOUR CODE HERE\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image /= 255.0\n",
        "  image = tf.image.resize(image, IMG_SIZE)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzvF61GV32k_"
      },
      "source": [
        "#### Preprocess the training set\n",
        "\n",
        "Use the `map()` and pass in the method that is just defined to preprocess the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpNEfDKM353a"
      },
      "outputs": [],
      "source": [
        "augmented_training_data = train_examples.map(augmentimages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4nFaMIMbrvA"
      },
      "source": [
        "#### Create batches of the training set.\n",
        "\n",
        "```Python\n",
        "# Shuffle the data if you're working on your own personal project\n",
        "train_batches = augmented_training_data.shuffle(1024).batch(32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POhDDPBY3vnL"
      },
      "outputs": [],
      "source": [
        "train_batches = augmented_training_data.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za5HxgT1_Cw6"
      },
      "source": [
        "### Build the Cats vs Dogs classifier\n",
        "\n",
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoyCA80GBSlG"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3),\n",
        "                        input_shape=(IMG_SIZE+(3,)),activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktnATyllHXC4"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```txt\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 300, 300, 16)      448       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 150, 150, 16)      0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 150, 150, 32)      4640      \n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 75, 75, 32)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 75, 75, 64)        18496     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2 (None, 37, 37, 64)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 37, 37, 128)       73856     \n",
        "_________________________________________________________________\n",
        "global_average_pooling2d (Gl (None, 128)               0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 2)                 258       \n",
        "=================================================================\n",
        "Total params: 97,698\n",
        "Trainable params: 97,698\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6nou82P_b5d"
      },
      "source": [
        "### Create a function to generate the saliency map\n",
        "\n",
        "Saliency Map Function the `do_salience()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKbvh3bl9vnG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework.importer import import_graph_def\n",
        "def do_salience(image, model, label, prefix):\n",
        "  '''\n",
        "  Generates the saliency map of a given image.\n",
        "\n",
        "  Args:\n",
        "    image (file) -- picture that the model will classify\n",
        "    model (keras Model) -- your cats and dogs classifier\n",
        "    label (int) -- ground truth label of the image\n",
        "    prefix (string) -- prefix to add to the filename of the saliency map\n",
        "  '''\n",
        "\n",
        "  img = cv2.imread(image)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, IMG_SIZE) / 255.0\n",
        "\n",
        "  tensor_image = np.expand_dims(img, axis=0)\n",
        "  number_of_classes = num_classes\n",
        "  one_hot_label = tf.one_hot([label] * tensor_image.shape[0], num_classes)\n",
        "\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    inputs = tf.cast(tensor_image, dtype=tf.float32)\n",
        "\n",
        "    # watch the input pixels\n",
        "    tape.watch(inputs)\n",
        "\n",
        "    predictions = model(inputs)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(one_hot_label, predictions)\n",
        "\n",
        "\n",
        "  gradients = tape.gradient(loss, inputs)\n",
        "  grayscale_tensor = tf.reduce_sum(tf.abs(gradients), axis=-1)\n",
        "  normalized_tensor = 255 * (grayscale_tensor - tf.reduce_min(grayscale_tensor)) / (tf.reduce_max(grayscale_tensor) - tf.reduce_min(grayscale_tensor))\n",
        "  normalized_tensor = tf.cast(normalized_tensor, dtype=tf.uint8)\n",
        "  normalized_tensor = tf.squeeze(normalized_tensor)\n",
        "\n",
        "\n",
        "  # plot the normalized tensor\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(normalized_tensor, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "  # saved\n",
        "  salient_image_name = prefix + image\n",
        "  normalized_tensor = tf.expand_dims(normalized_tensor, -1)\n",
        "  normalized_tensor = tf.io.encode_jpeg(normalized_tensor, quality=100, format='grayscale')\n",
        "  writer = tf.io.write_file(salient_image_name, normalized_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li1idRy-parp"
      },
      "source": [
        "### Generate saliency maps with untrained model\n",
        "\n",
        "\n",
        "Applying the `do_salience()` function on the following image files:\n",
        "\n",
        "* `cat1.jpg`\n",
        "* `cat2.jpg`\n",
        "* `catanddog.jpg`\n",
        "* `dog1.jpg`\n",
        "* `dog2.jpg`\n",
        "\n",
        "Cats will have the label `0` while dogs will have the label `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k39fF4n8fgG0"
      },
      "outputs": [],
      "source": [
        "# load initial weights\n",
        "model.load_weights('0_epochs.h5')\n",
        "\n",
        "# generate the saliency maps for the 5 test images\n",
        "# YOUR CODE HERE\n",
        "do_salience('cat1.jpg', model, 0, 'epoch0_salient')\n",
        "do_salience('cat2.jpg', model, 0, 'epoch0_salient')\n",
        "do_salience('catanddog.jpg', model, 0, 'epoch0_salient')\n",
        "do_salience('dog1.jpg', model, 1, 'epoch0_salient')\n",
        "do_salience('dog2.jpg', model, 1, 'epoch0_salient')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kcdyut5E2Tk"
      },
      "source": [
        "With untrained weights, we will see something like this in the output.\n",
        "- we will see strong pixels outside the cat that the model uses that when classifying the image.\n",
        "- After training that these will slowly start to localize to features inside the pet.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1h5wP52lwbBUMVLlsgyb-tQl_I9eu42X7' alt='saliency'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZhZgd0x_JvN"
      },
      "source": [
        "### Configure the model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkyWZ5KdBo-z"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "rmsprop = RMSprop(learning_rate=0.001)\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = rmsprop,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YSNp7k7BqfL"
      },
      "outputs": [],
      "source": [
        "# load pre-trained weights\n",
        "model.load_weights('15_epochs.h5')\n",
        "\n",
        "# train the model for just 3 epochs\n",
        "# YOUR CODE HERE\n",
        "model.fit(train_batches,\n",
        "          epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tTqtLN3tQJx"
      },
      "source": [
        "### Generate saliency maps at 18 epochs\n",
        "\n",
        "We use your `do_salience()` function again on the same test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFtabyVhIKN"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "do_salience('cat1.jpg', model, 0, 'salient')\n",
        "do_salience('cat2.jpg', model, 0, 'salient')\n",
        "do_salience('catanddog.jpg', model, 0, 'salient')\n",
        "do_salience('dog1.jpg', model, 1, 'salient')\n",
        "do_salience('dog2.jpg', model, 1, 'salient')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPtx-u4u_jL5"
      },
      "source": [
        "### Zip the images for grading\n",
        "\n",
        "Zip the normalized tensor images you generated at 18 epochs\n",
        "\n",
        "* salientcat1.jpg\n",
        "* salientcat2.jpg\n",
        "* salientcatanddog.jpg\n",
        "* salientdog1.jpg\n",
        "* salientdog2.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-MhcA8Uh8H_"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "!rm images.zip\n",
        "\n",
        "filenames = ['cat1.jpg', 'cat2.jpg', 'catanddog.jpg', 'dog1.jpg', 'dog2.jpg']\n",
        "\n",
        "# writing files to a zipfile\n",
        "with ZipFile('images.zip','w') as zip:\n",
        "  for file in filenames:\n",
        "    zip.write('salient' + file)\n",
        "\n",
        "print(\"images.zip generated!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}